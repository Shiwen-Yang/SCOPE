{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ecbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.distributions import Dirichlet, Normal\n",
    "from src import loss_functions as LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "81700ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Gpp_basis(g_prime, g_double_prime, B1, B2, Y, sigma = 0.02):\n",
    "    \"\"\"\n",
    "    Compute empirical estimate of E[(B1 y)^T g''(y) B2 y - g'(y)^T B1^T B2 y]\n",
    "\n",
    "    Args:\n",
    "        g_prime: function (N x p) -> (N x p), gradient function\n",
    "        g_double_prime: function (N x p) -> (N x p x p), Hessian function\n",
    "        B1: skew-symmetric matrix (p x p)\n",
    "        B2: skew-symmetric matrix (p x p)\n",
    "        Y: data matrix (N x p)\n",
    "\n",
    "    Returns:\n",
    "        Scalar: empirical estimate of second derivative contribution\n",
    "    \"\"\"\n",
    "    B1 = B1.float()\n",
    "    B2 = B2.float()\n",
    "    Y = Y.float()\n",
    "\n",
    "    B1Y = Y @ B1.T  # (N x p)\n",
    "    B2Y = Y @ B2.T  # (N x p)\n",
    "\n",
    "    # Hessian term: (B1 y)^T H (B2 y)\n",
    "    H = g_double_prime(Y, sigma)        # (N x p x p)\n",
    "    B2Y_exp = B2Y.unsqueeze(2)   # (N x p x 1)\n",
    "    HB2Y = torch.bmm(H, B2Y_exp) # (N x p x 1)\n",
    "    term1 = torch.bmm(B1Y.unsqueeze(1), HB2Y).squeeze()  # (N,)\n",
    "    \n",
    "    # Gradient term: g'(y)^T B1^T B2 y\n",
    "    B1B2Y = Y @ (B2.T @ B1).T  # (N x p)\n",
    "    gY = g_prime(Y, sigma)            # (N x p)\n",
    "    term2 = (gY * B1B2Y).sum(dim=1)  # (N,)\n",
    "\n",
    "    return (term1 - term2).mean().item()\n",
    "\n",
    "\n",
    "def mollified_relu_grad(Y, sigma=0.02):\n",
    "    \"\"\"\n",
    "    Compute gradient of mollified ReLU loss w.r.t. Y.\n",
    "    Y: (N x p), sigma: scalar\n",
    "    Returns: (N x p) gradient matrix\n",
    "    \"\"\"\n",
    "    normal = Normal(0, 1)\n",
    "    N, p = Y.shape\n",
    "\n",
    "    # Negative term gradient (for smoothed_relu(-x_j))\n",
    "    grad_neg = -normal.cdf(-Y / sigma)  # shape: (N x p)\n",
    "\n",
    "    # Sum constraint gradient (for smoothed_relu(sum(x) - 1))\n",
    "    w = Y.sum(dim=1, keepdim=True) - 1  # shape: (N x 1)\n",
    "    grad_sum = normal.cdf(w / (sigma * p**0.5))  # shape: (N x 1)\n",
    "    grad_sum_expanded = grad_sum.expand(-1, p)\n",
    "\n",
    "    return grad_neg + grad_sum_expanded\n",
    "\n",
    "\n",
    "# def mollified_relu_hess(Y, sigma=0.02):\n",
    "#     \"\"\"\n",
    "#     Computes the manual Hessian of the mollified ReLU simplex loss.\n",
    "#     Args:\n",
    "#         Y: (N, p) batch of data points\n",
    "#         sigma: smoothing parameter\n",
    "\n",
    "#     Returns:\n",
    "#         Hessian: tensor of shape (N, p, p)\n",
    "#     \"\"\"\n",
    "#     N, p = Y.shape\n",
    "#     device = Y.device\n",
    "#     normal = Normal(0.0, 1.0)\n",
    "\n",
    "#     # First term: ReLU(-x_i) for each coordinate (diagonal Hessian)\n",
    "#     z_neg = -Y / sigma  # shape (N, p)\n",
    "#     phi_neg = normal.log_prob(z_neg).exp()  # PDF\n",
    "#     H_neg = torch.diag_embed(phi_neg / sigma)  # shape (N, p, p)\n",
    "\n",
    "#     # Second term: ReLU(sum(x_i) - 1) is rank-1\n",
    "#     sum_x = Y.sum(dim=1, keepdim=True)  # shape (N, 1)\n",
    "#     z_sum = (sum_x - 1) / (sigma * p**0.5)\n",
    "#     phi_sum = normal.log_prob(z_sum).exp()  # shape (N, 1)\n",
    "#     coeff = phi_sum / (sigma * p**0.5)  # shape (N, 1)\n",
    "\n",
    "#     ones = torch.ones((N, p, 1), device=device)\n",
    "#     H_sum = coeff.view(-1, 1, 1) * torch.bmm(ones, ones.transpose(1, 2))  # shape (N, p, p)\n",
    "\n",
    "#     return H_neg + H_sum\n",
    "def mollified_relu_hess(y, sigma = 0.02):\n",
    "    \n",
    "    p = torch.tensor(y.shape[0])\n",
    "    device = y.device\n",
    "    normal = Normal(0, 1)\n",
    "    \n",
    "    log_phi = normal.log_prob\n",
    "    \n",
    "    term_1 = torch.exp(log_phi(y/sigma))/sigma\n",
    "    print(term_1)\n",
    "    term_2 = torch.exp(log_phi((torch.sum(y)-1)/(torch.sqrt(p)*sigma)))/(torch.sqrt(p)*sigma)\n",
    "    print(term_2)\n",
    "    term_1 = torch.diag(term_1)\n",
    "    term_2 = term_2 * torch.ones((p, p), device=device)\n",
    "    \n",
    "    return(term_1 + term_2)\n",
    "\n",
    "def skew_basis(p, device = \"cuda\", dtype = torch.float32):\n",
    "    basis = []\n",
    "    for i in range(p):\n",
    "        for j in range(i + 1, p):\n",
    "            B = torch.zeros(p, p, device=device, dtype=dtype)\n",
    "            B[i, j] = 1 / math.sqrt(2)\n",
    "            B[j, i] = -1 / math.sqrt(2)\n",
    "            basis.append(B)\n",
    "    return torch.stack(basis)  # (d, p, p)\n",
    "\n",
    "def construct_M_from_basis(g_prime, g_double_prime, B, Y):\n",
    "    \"\"\"\n",
    "    Construct M matrix using basis elements B and gradient/Hessian functions.\n",
    "\n",
    "    Args:\n",
    "        g_prime: grad function (N x p) -> (N x p)\n",
    "        g_double_prime: hessian function (N x p) -> (N x p x p)\n",
    "        B: tensor of skew basis matrices (d x p x p)\n",
    "        Y: data matrix (N x p)\n",
    "\n",
    "    Returns:\n",
    "        M: tensor of shape (d x d)\n",
    "    \"\"\"\n",
    "    d = B.shape[0]\n",
    "    M = torch.zeros(d, d, device=Y.device, dtype=Y.dtype)\n",
    "\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            M[i, j] = compute_Gpp_basis(g_prime, g_double_prime, B[i], B[j], Y)\n",
    "\n",
    "    return M\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f36e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 6\n",
    "\n",
    "alpha = torch.ones(p, p, dtype= torch.float32) + torch.eye(p, dtype= torch.float32)*0\n",
    "# alpha = torch.tensor([[10, 1, 1, 1], [1, 10, 1], [1, 1, 10]], dtype= torch.float64)\n",
    "# alpha = torch.tensor([[5, 1]], dtype= torch.float64)\n",
    "n= 30000\n",
    "K, p = alpha.shape\n",
    "torch.manual_seed(5)\n",
    "dir = Dirichlet(alpha)\n",
    "X = dir.sample((n // K,)).transpose(0, 1).reshape(n, p)[:, :p].to(\"cuda\")\n",
    "\n",
    "B = skew_basis(p, \"cuda\")\n",
    "M = construct_M_from_basis(mollified_relu_grad, mollified_relu_hess, B, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211cd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = torch.linalg.eigh(M)\n",
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8c88bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import LossFunctionWrapper\n",
    "import torch\n",
    "import math\n",
    "from torch.distributions import Dirichlet, Normal\n",
    "from src import loss_functions as LF\n",
    "\n",
    "def smoothed_relu(z, sigma):\n",
    "    \"\"\"\n",
    "    Smooth approximation of ReLU using Gaussian mollifier.\n",
    "\n",
    "    Args:\n",
    "        z: tensor\n",
    "        sigma: smoothing parameter (scalar or tensor broadcastable to z)\n",
    "\n",
    "    Returns:\n",
    "        Smoothed ReLU approximation, same shape as z\n",
    "    \"\"\"\n",
    "    sqrt_2 = math.sqrt(2)\n",
    "    sqrt_2pi = math.sqrt(2 * math.pi)\n",
    "\n",
    "    scaled = z / sigma\n",
    "    phi = torch.exp(-0.5 * scaled**2) / sqrt_2pi\n",
    "    Phi = 0.5 * (1 + torch.erf(scaled / sqrt_2))\n",
    "\n",
    "    return sigma * phi + z * Phi\n",
    "\n",
    "def mollified_relu_simplex_core(x, sigma):\n",
    "    \"\"\"\n",
    "    Smooth version of the ReLU simplex loss using smoothed ReLU.\n",
    "\n",
    "    Args:\n",
    "        x: (n, p) tensor\n",
    "        sigma: smoothing parameter (float or tensor)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n,) with smoothed loss values\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    \n",
    "    n, p = x.shape\n",
    "    sigma = torch.as_tensor(sigma, dtype=x.dtype, device=x.device)\n",
    "    p = torch.tensor(p, device=x.device)\n",
    "    \n",
    "    # Negative entries penalty\n",
    "    neg_part = smoothed_relu(-x, sigma).sum(dim=1)\n",
    "\n",
    "    # Sum constraint penalty\n",
    "    sum_constraint = torch.sum(x) - 1  # penalize when > 1\n",
    "    sum_part = smoothed_relu(sum_constraint, sigma*torch.sqrt(p))\n",
    "\n",
    "    return neg_part + sum_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2b4fbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = LossFunctionWrapper(mollified_relu_simplex_core)\n",
    "\n",
    "p = 6\n",
    "\n",
    "alpha = torch.ones(p, p, dtype= torch.float32) + torch.eye(p, dtype= torch.float32)*0\n",
    "# alpha = torch.tensor([[10, 1, 1, 1], [1, 10, 1], [1, 1, 10]], dtype= torch.float64)\n",
    "# alpha = torch.tensor([[5, 1]], dtype= torch.float64)\n",
    "n= 30000\n",
    "K, p = alpha.shape\n",
    "torch.manual_seed(5)\n",
    "dir = Dirichlet(alpha)\n",
    "X = dir.sample((n // K,)).transpose(0, 1).reshape(n, p)[:, :p].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "01fbd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "H1 = temp.hessian(X[0, :], 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7b881ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0487e-01, 1.7785e+00, 6.8381e-03, 0.0000e+00, 3.9452e-22, 8.5327e+00],\n",
      "       device='cuda:0')\n",
      "tensor(8.1434, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "H2 = mollified_relu_hess(X[0, :], 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8ed381c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00, -2.8610e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1 - H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a00b7258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20.5520, 19.9471, 19.9471, 19.9471, 19.9471, 19.9471],\n",
      "        [19.9471, 21.7256, 19.9471, 19.9471, 19.9471, 19.9471],\n",
      "        [19.9471, 19.9471, 19.9540, 19.9471, 19.9471, 19.9471],\n",
      "        [19.9471, 19.9471, 19.9471, 19.9471, 19.9471, 19.9471],\n",
      "        [19.9471, 19.9471, 19.9471, 19.9471, 19.9471, 19.9471],\n",
      "        [19.9471, 19.9471, 19.9471, 19.9471, 19.9471, 28.4798]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = X[0, :]\n",
    "x.requires_grad_(True)\n",
    "loss = mollified_relu_simplex_core(x, 0.02)\n",
    "grad = torch.autograd.grad(loss, x, create_graph=True)[0]\n",
    "\n",
    "# Second derivative (Hessian)\n",
    "p = x.numel()\n",
    "hessian = torch.zeros(p, p, dtype=x.dtype, device=x.device)\n",
    "\n",
    "for i in range(p):\n",
    "    # Compute gradient of grad[i] w.r.t. x again\n",
    "    grad2 = torch.autograd.grad(grad[i], x, retain_graph=True)[0]\n",
    "    hessian[i] = grad2\n",
    "print(hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "29a7877a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6049, device='cuda:0')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[0, 0]\n",
    "x.requires_grad_(True)\n",
    "loss = smoothed_relu(x, 0.02)\n",
    "grad = torch.autograd.grad(loss, x, create_graph=True)[0]\n",
    "grad.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "673eec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9959, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5cad7337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4959, 0.4861, 0.5000, 0.5000, 0.5000, 0.4037]], device='cuda:0')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mollified_relu_grad(X[0, :].unsqueeze(dim = 0), 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebce18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4959, 0.4861, 0.5000, 0.5000, 0.5000, 0.4037], device='cuda:0')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767bf493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
